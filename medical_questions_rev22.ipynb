{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cf21ca",
   "metadata": {},
   "source": [
    "*Привет!*\n",
    "\n",
    "*Меня зовут Фёдор, я буду проводить ревью твоего проекта.*\n",
    "\n",
    "*Предлагаю общаться на «ты» :)*\n",
    "\n",
    "*Моя основная цель — не указать на совершенные тобою ошибки, а поделиться своим опытом и помочь тебе стать отличным специалистом. Ниже ты найдешь мои комментарии, оформленные вот так: «Комментарий ревьюера:…»*\n",
    "\n",
    "*Помни, что все мои комментарии являются рекомендациями, а не требованиями – только ты решаешь, как сделать этот проект лучше.*\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Комментарий ревьюера ❌:</b> </div>\n",
    "    \n",
    "Так выделены самые важные замечания. Их нужно обязательно отработать, перед тем как показывать работу другим специалистам по Data Science.\n",
    "    \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Комментарий ревьюера ⚡:</b> </div>\n",
    "    \n",
    "Так выделены небольшие замечания. Я надеюсь, что их ты тоже учтешь - твой проект от этого станет только лучше.\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Комментарий ревьюера ✅:</b> </div>\n",
    "    \n",
    "Так я выделяю правильные и верные действия, а так же все остальные комментарии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2070a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Комментарии к приложению✅:</b> \n",
    "\n",
    "- requirements - проверь, пожалуйста, не попало ли туда чего-то лишнего. подозреваю, что, например, виджеты или jupyter не являются необходимыми компонентами именно для сборки приложения\n",
    "- работающее приложение - супер-круто, молодец! Но обрати, пожалуйста, внимание: я столкнулся с тем, что при вводе нового (пользовательского) запроса возвращается тот же результат, что и к дефолтному (про месячные). Как будто бы какая-то переменная не обновляется.\n",
    "    \n",
    "- **translate.py** Классный подход с переводом. Еще можно было бы не использовать LLM, а обращаться к тому же google translate, например, так\n",
    "```python\n",
    "    from deep_translator import GoogleTranslator\n",
    "    translator = GoogleTranslator(source='en', target='ru')\n",
    "    v = 'some text'\n",
    "    tv = translator.translate(v)\n",
    "```\n",
    "- **dataload.py** не смотря на то, что датасет открытый, хранить его в репозитории все рано не стоит. А при except рекомендуется указывать [тип ожидаемой ошибки](https://www.w3schools.com/python/python_try_except.asp), потому что разные ошибки могут требовать разного поведения приложения\n",
    "- **prediction.py**. строка 15. Кажется, хардкодить N - не лучшая идея\n",
    "\n",
    "- **dataload.py**. Если скачиваем датасет - лучше это делать через requests, скачивать файл во временную папку, а затем оттуда читать. Это позволит обрабатывать ошибки при обращении к веб-странице. А еще безопаснее было бы давать путь к файлу \n",
    "    ```python\n",
    "    os.path.join(os.dirname(__file__), DATA_PATH)\n",
    "    ```\n",
    "Дело в том, что написанный тобой вариант не сработает, если изменится точка запуска скрипта dataload.py. То есть, если находится в той же папке, что и скрипт и запускать его через \n",
    "    ```bash\n",
    "    python3 dataload.py\n",
    "    ```\n",
    "    все будет хорошо, а вот если запускать откуда-то еще - \n",
    "    ```bash\n",
    "    python3 some_dir/dataload.py\n",
    "    ```\n",
    "    путь к файлу потеряется\n",
    "\n",
    "    \n",
    "А теперь посмотрим на исследовательскую часть\n",
    "\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e302cbef-1fbb-483b-a49a-52c2315fade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sumkin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sumkin/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/sumkin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sumkin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/sumkin/nltk_data...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "#from scipy.spatial import distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"Mean of empty slice\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d7edd-66ab-4423-b517-09fdc0fafdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_on_dataset(_df):\n",
    "    CONST_IN_PERCENTAGES = float(100.0)\n",
    "    try:\n",
    "        display(_df.head())\n",
    "        print(\"\")\n",
    "        _df.info()\n",
    "        df_review_info = pd.DataFrame({'Not_Valid_Values_in %': (_df.isna().sum()/len(_df))*CONST_IN_PERCENTAGES} )\n",
    "        print(\"\")\n",
    "        display(df_review_info.sort_values(by='Not_Valid_Values_in %', ascending=False))\n",
    "        print(\"\")\n",
    "        print(\"Number of duplicated rows in the dataset: {}\".format(_df.duplicated().sum()))\n",
    "        print(\"\")\n",
    "        print(\"Number of unique values in the dataset:\\n{}\".format(_df.nunique()))\n",
    "    except:\n",
    "        print(\"Can't provide review on given dataset\")\n",
    "\n",
    "def adjust_total_stop_words_list(_stop_words_list):\n",
    "    total_stop_words_sample = ' '.join(_stop_words_list)\n",
    "    total_stop_words_sample_processed = unicodedata.normalize('NFKD',\n",
    "                                                              total_stop_words_sample).encode('ascii','ignore').decode()\n",
    "    total_stop_words_sample_processed = punctuation_and_case(total_stop_words_sample_processed)\n",
    "    total_stop_words_list_processed = total_stop_words_sample_processed.split(\" \")\n",
    "    return total_stop_words_list_processed\n",
    "\n",
    "def punctuation_and_case(text_sample):\n",
    "    string_punctuation = string.punctuation.replace('-','') + \"’\"\n",
    "    punctuation_map = str.maketrans(string_punctuation, ' '*len(string_punctuation))\n",
    "    return text_sample.translate(punctuation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51587645-09ff-42ef-965d-d3238676ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_LANGUAGES_LIST_CONST = ['english'] \n",
    "total_stop_words_lists = [stopwords.words(_lng) for _lng in STOP_WORDS_LANGUAGES_LIST_CONST]\n",
    "total_stop_words_list = list(itertools.chain(*total_stop_words_lists))\n",
    "TOTAL_STOP_WORDS_LIST_PROCESSED_CONST = adjust_total_stop_words_list(total_stop_words_list)\n",
    "TF_IDF_MIN_DF_VALUE_CONST = int(3)\n",
    "TF_IDF_MAX_DF_VALUE_CONST = float(0.5)\n",
    "MAX_LENGTH = 100\n",
    "NUM_BEAMS = 3\n",
    "EARLY_STOP = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "model_translate = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "\n",
    "model_translate_glaz = \"glazzova/ml_translation_model1\"\n",
    "translator = pipeline(\"translation\", model=model_translate_glaz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6464834-5634-4dd6-ad61-16b2607b5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering(text_sample):\n",
    "    processed_words_list = [w.lower() for w in text_sample.split()]\n",
    "    return ' '.join(processed_words_list)\n",
    "\n",
    "def stop_words(text_sample, _stop_words_ = TOTAL_STOP_WORDS_LIST_PROCESSED_CONST):\n",
    "    text_sample_processed = [word for word in text_sample.split() if word not in _stop_words_]\n",
    "    return ' '.join(text_sample_processed)\n",
    "\n",
    "def strip_hashtags(text_sample):\n",
    "    processed_words_list = []\n",
    "    for word in text_sample.split(' '):\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word.find('@') < 0 and word.find('#') < 0 :\n",
    "                processed_words_list.append(word)\n",
    "    return ' '.join(processed_words_list)\n",
    "\n",
    "def alphanumeric_adjustment(text_sample):\n",
    "    def if_word_valid(w):\n",
    "        if w.isalnum() and not w.isalpha() and not w.isdigit():\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    processed_words_list = [word for word in text_sample.split() if if_word_valid(word)]\n",
    "    return ' '.join(processed_words_list)\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemm_sample(text_sample):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text_sample))\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    text_sample_processed = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            text_sample_processed.append(word)\n",
    "        else:\n",
    "            text_sample_processed.append(wnl.lemmatize(word, tag))\n",
    "    return ' '.join(text_sample_processed)\n",
    "\n",
    "def text_sample_lem_processing(text_sample, _stop_words_remove=False):\n",
    "    text_sample_processed = unicodedata.normalize('NFKD', text_sample).encode('ascii','ignore').decode()\n",
    "    text_sample_processed = strip_hashtags(text_sample_processed)\n",
    "    text_sample_processed = punctuation_and_case(text_sample_processed)\n",
    "    text_sample_processed = lowering(text_sample_processed)\n",
    "    if _stop_words_remove:\n",
    "        text_sample_processed = stop_words(text_sample_processed)\n",
    "    text_sample_processed = lemm_sample(text_sample_processed)\n",
    "    text_sample_processed = alphanumeric_adjustment(text_sample_processed)\n",
    "    return text_sample_processed\n",
    "\n",
    "def translate_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    translated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        outputs = model_translate.generate(inputs, max_length=MAX_LENGTH, num_beams=NUM_BEAMS, early_stopping=EARLY_STOP)\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translated_sentences.append(translated_text)\n",
    "    translated_text = \" \".join(translated_sentences)\n",
    "    return translated_text\n",
    "\n",
    "def translate_text_glaz(text):\n",
    "    return translator(text)[0]['translation_text']\n",
    "\n",
    "def calculate_cosine_similarity(v1, v2):\n",
    "    emb1 = model.encode(v1, convert_to_tensor=True)\n",
    "    emb2 = model.encode(v2, convert_to_tensor=True)\n",
    "    cos_sim = util.cos_sim(emb1, emb2)\n",
    "    return cos_sim.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85634e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Тут лучше было бы делать функции более универсальными, чтобы можно было в качестве параметра передавать модель (разные модели для перевода) или разные токенайзеры ⚡:</b> </div>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "os.makedirs(f'../{DATA_DIR}', exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(\"medical_questions_pairs\")\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "train_df.to_csv(f'../{DATA_DIR}/train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae68008",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info_on_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491019a0-0324-4d78-bba4-62cd7dda567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()#normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd062715-95de-41d2-aff3-dbc3aab995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv(f'../{DATA_DIR}/train_df_processed.csv', index_col=[0])\n",
    "\n",
    "except:\n",
    "    train_df['question_1_processed'] = train_df['question_1'].map(lambda _sample: text_sample_lem_processing(_sample, _stop_words_remove=True))\n",
    "    train_df['question_2_processed'] = train_df['question_2'].map(lambda _sample: text_sample_lem_processing(_sample, _stop_words_remove=True))\n",
    "    train_df['len_question_1'] = train_df['question_1'].map(lambda x: len(x)) \n",
    "    train_df['len_question_2'] = train_df['question_2'].map(lambda x: len(x)) \n",
    "    train_df['text_len_question_1'] = train_df['question_1_processed'].map(lambda x: len(x)) \n",
    "    train_df['text_len_question_2'] = train_df['question_2_processed'].map(lambda x: len(x)) \n",
    "    train_df['rus_question_1'] = train_df['question_1'].apply(translate_text)\n",
    "    train_df['rus_question_2'] = train_df['question_2'].apply(translate_text)\n",
    "    train_df['rusgl_question_1'] = train_df['question_1'].apply(translate_text_glaz)\n",
    "    train_df['rusgl_question_2'] = train_df['question_2'].apply(translate_text_glaz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a42ece",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Вот тут не хватает пояснений, что делает эта конструкция. Если не получилось прочитать файл по заданному пути - то - что? ⚡:</b> </div>\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506704c8-f8c9-4f57-b753-e8fb668a62dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d53a8a-7e29-4b1d-8ede-5764ea701ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_info_on_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eee94-2914-4be7-b642-2fddbddd564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['question_1_processed','question_1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f823054-93f8-4443-9bcf-30eab683889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['question_2_processed','question_2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc34519-81e8-4ae3-960c-9c9fc7ff56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df[train_df['len_question_1'] <= 0]))\n",
    "print(len(train_df[train_df['len_question_2'] <= 0]))\n",
    "print(len(train_df[train_df['text_len_question_1'] <= 0]))\n",
    "print(len(train_df[train_df['text_len_question_2'] <= 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e76285",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Кажется, проверка на отрицательную длину предложения будет всегда давать один и тот же результат. А вот отбросить слишком длинные или слишком короткие предлодения - можно было бы ⚡:</b> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de2663-2f13-4add-b526-863a50692492",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa487c8-6964-4ec9-ba8e-9dc95b679eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_translate = ['question_1','question_2']\n",
    "translated_columns = ['rus_question_1','rus_question_2']\n",
    "train_df[translated_columns+column_to_translate+['label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b42d6",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Классный подход!✅:</b> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0843a50-05c0-49ab-8f2b-94c72b880c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c32d7-4a49-4e4f-b23a-66b3e9cd24a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f32a8-4592-44a0-b9fc-b96c3f00d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.concat([train_df['question_1_processed'],train_df['question_2_processed']]).drop_duplicates(keep='first')\n",
    "corpus.head(), corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fd93e-748e-4dc5-8059-3b2c710375ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full = pd.concat([train_df['question_1'],train_df['question_2']]).drop_duplicates(keep='first')\n",
    "corpus_full.head(), corpus_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cacf6-1d7f-4fc0-93b9-3a2d7abf8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74652e-91d8-4692-b588-9b34b838d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_corpus = tf_idf_vectorizer.fit_transform(corpus)\n",
    "tf_idf_q1 = tf_idf_vectorizer.transform(train_df['question_1_processed'])\n",
    "tf_idf_q2 = tf_idf_vectorizer.transform(train_df['question_2_processed'])\n",
    "tf_idf_q1_array = tf_idf_q1.toarray()\n",
    "tf_idf_q2_array = tf_idf_q2.toarray()\n",
    "tf_idf_corpus_array = tf_idf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79472833-f560-4907-99d5-cb55eb7319b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_corpus_full = tf_idf_vectorizer.fit_transform(corpus_full)\n",
    "tf_idf_q1_full = tf_idf_vectorizer.transform(train_df['question_1'])\n",
    "tf_idf_q2_full = tf_idf_vectorizer.transform(train_df['question_2'])\n",
    "tf_idf_q1_full_array = tf_idf_q1_full.toarray()\n",
    "tf_idf_q2_full_array = tf_idf_q2_full.toarray()\n",
    "tf_idf_corpus_full_array = tf_idf_corpus_full.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4b266-1276-4c72-82b8-58b83d647ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f64b62-903a-4bbb-b632-fe562779945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc6bdc-aa0c-486c-aaa4-41c1a7728400",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_q1_ = pd.Series(tf_idf_q1_array.tolist())\n",
    "tf_idf_q2_ = pd.Series(tf_idf_q2_array.tolist())\n",
    "train_df['tf_idf_q1'] = tf_idf_q1_\n",
    "train_df['tf_idf_q2'] = tf_idf_q2_\n",
    "train_df['cdist_cb'] = [cdist([row['tf_idf_q1']], [row['tf_idf_q2']],'cityblock')[0][0] for _, row in train_df.iterrows()]\n",
    "train_df['cdist_eu'] = [cdist([row['tf_idf_q1']], [row['tf_idf_q2']],'euclidean')[0][0] for _, row in train_df.iterrows()]\n",
    "\n",
    "tf_idf_q1_full_ = pd.Series(tf_idf_q1_full_array.tolist())\n",
    "tf_idf_q2_full_ = pd.Series(tf_idf_q2_full_array.tolist())\n",
    "train_df['tf_idf_q1_full'] = tf_idf_q1_full_\n",
    "train_df['tf_idf_q2_full'] = tf_idf_q2_full_\n",
    "train_df['cdist_cb_full'] = [cdist([row['tf_idf_q1_full']], [row['tf_idf_q2_full']],'cityblock')[0][0] for _, row in train_df.iterrows()]\n",
    "train_df['cdist_eu_full'] = [cdist([row['tf_idf_q1_full']], [row['tf_idf_q2_full']],'euclidean')[0][0] for _, row in train_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e464c44-a216-40c9-9259-ed0170031096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['question_1','question_2','label','cdist_cb','cdist_eu','cdist_cb_full','cdist_eu_full']].head()#.query('question_1 == @s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ffa3d-82b2-4c18-bd40-5672f4622e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings1 = model.encode(train_df['question_1'].tolist())\n",
    "embeddings2 = model.encode(train_df['question_2'].tolist())\n",
    "train_df['question_1_mLM'] = embeddings1.tolist()\n",
    "train_df['question_2_mLM'] = embeddings2.tolist()\n",
    "train_df['cos_sim_mLM'] = train_df.apply(lambda x: calculate_cosine_similarity(x['question_1'], x['question_2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ac218-c3cb-4cb7-9d28-973c8753a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['question_1','question_2','label','cdist_cb','cdist_eu','cos_sim_mLM']].query('label == 1').sort_values(by='cos_sim_mLM', ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11a3f4-f94a-4b37-8d81-c07ea7fc6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['question_1','question_2','label','cdist_cb','cdist_eu','cos_sim_mLM']].query('label == 1').sort_values(by='cos_sim_mLM', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70934e7-8a07-4375-ad56-06b8d84be0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688a492-2d01-4b9e-90e3-8cc4be058a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cols = ['cdist_cb','cdist_eu','cdist_cb_full','cdist_eu_full']\n",
    "scaled_cols = ['cdist_cb_scl','cdist_eu_scl','cdist_cb_full_scl','cdist_eu_full_scl']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = pd.DataFrame(scaler.fit_transform(train_df[scaler_cols]), columns=scaled_cols)\n",
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de3640-45e3-485c-9532-395ff1c15e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df,scaled_data],axis=1)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbcda5-8c28-439d-8081-85bca4e9fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.query('label == 0').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfc7d6-5ded-4dab-a750-3ef324ca9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.query('label == 1').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9dbe0-1216-4161-83f8-d171d323b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_topN(question_1, question_2, def_predict, metric_=None, N=5):\n",
    "\n",
    "    corr_pred = 0\n",
    "    all_pred = 0\n",
    "    \n",
    "    question_1_array = np.vstack(question_1)\n",
    "    question_2_array = np.vstack(question_2)\n",
    "\n",
    "    if def_predict=='cdist_':\n",
    "        predict_1 = np.array([cdist(question_2_array, np.expand_dims(q1, axis=0), metric=metric_).reshape(-1) for q1 in question_1_array])\n",
    "        predict_2 = np.array([cdist(question_1_array, np.expand_dims(q2, axis=0), metric=metric_).reshape(-1) for q2 in question_2_array])\n",
    "    elif def_predict=='cos_sim_':\n",
    "        predict_1 = np.array([util.cos_sim(question_2_array, q1).reshape(-1) for q1 in question_1_array])\n",
    "        predict_2 = np.array([util.cos_sim(question_1_array, q2).reshape(-1) for q2 in question_2_array])\n",
    "        \n",
    "    for i in range(len(question_1_array)):\n",
    "        values_top = np.argsort(predict_1[i])[::-1][:N]  \n",
    "        if i in values_top:\n",
    "            corr_pred += 1\n",
    "        all_pred += 1\n",
    "    \n",
    "    for i in range(len(question_2_array)):\n",
    "        values_top = np.argsort(predict_2[i])[::-1][:N]\n",
    "        if i in values_top:\n",
    "            corr_pred += 1\n",
    "        all_pred += 1\n",
    "    \n",
    "    accuracy_top = corr_pred / all_pred\n",
    "    return accuracy_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1979a80-3ecf-4cde-8acb-dade81ac3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_top5_cos_sim = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cos_sim_', N=5)\n",
    "accuracy_top10_cos_sim = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cos_sim_', N=10)\n",
    "accuracy_top5_cos_sim, accuracy_top10_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aab04c-dd1d-4ffc-a6e7-49d53445ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_top5_cos_sim_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cos_sim_', N=5)\n",
    "accuracy_top10_cos_sim_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cos_sim_', N=10)\n",
    "accuracy_top5_cos_sim_tf, accuracy_top10_cos_sim_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2eb84",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Вот тут все хорошо, и метрика довольно высокая ✅:</b> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fcf90-c010-4bc8-8490-01be4be644b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_top5_cdist_cb = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cdist_', metric_='cityblock', N=5)\n",
    "accuracy_top10_cdist_cb = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cdist_', metric_='cityblock', N=10)\n",
    "accuracy_top5_cdist_cb, accuracy_top10_cdist_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b7a93-cc88-4037-97d2-4c5ee620a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_top5_cdist_eu = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cdist_', metric_='euclidean', N=5)\n",
    "accuracy_top10_cdist_eu = accuracy_topN(train_df['question_1_mLM'], train_df['question_2_mLM'], def_predict='cdist_', metric_='euclidean', N=10)\n",
    "accuracy_top5_cdist_eu, accuracy_top10_cdist_eu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91a495",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>А вот тут нужны пояснения, как трактовать такие значения accuracy ⚡:</b> </div>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5add2-3d3f-4494-b393-01b353861c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_top5_cdist_cb_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cdist_', metric_='cityblock', N=5)\n",
    "# accuracy_top10_cdist_cb_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cdist_', metric_='cityblock', N=10)\n",
    "# accuracy_top5_cdist_cb_tf, accuracy_top10_cdist_cb_tf\n",
    "\n",
    "# accuracy_top5_cdist_eu_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cdist_', metric_='euclidean', N=5)\n",
    "# accuracy_top10_cdist_eu_tf = accuracy_topN(train_df['tf_idf_q1'], train_df['tf_idf_q2'], def_predict='cdist_', metric_='euclidean', N=10)\n",
    "# accuracy_top5_cdist_eu_tf, accuracy_top10_cdist_eu_tf\n",
    "\n",
    "# accuracy_top5_cos_sim_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cos_sim_', N=5)\n",
    "# accuracy_top10_cos_sim_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cos_sim_', N=10)\n",
    "# accuracy_top5_cos_sim_tf_full, accuracy_top10_cos_sim_tf_full\n",
    "\n",
    "# accuracy_top5_cdist_cb_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cdist_', metric_='cityblock', N=5)\n",
    "# accuracy_top10_cdist_cb_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cdist_', metric_='cityblock', N=10)\n",
    "# accuracy_top5_cdist_cb_tf_full, accuracy_top10_cdist_cb_tf_full\n",
    "\n",
    "# accuracy_top5_cdist_eu_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cdist_', metric_='euclidean', N=5)\n",
    "# accuracy_top10_cdist_eu_tf_full = accuracy_topN(train_df['tf_idf_q1_full'], train_df['tf_idf_q2_full'], def_predict='cdist_', metric_='euclidean', N=10)\n",
    "# accuracy_top5_cdist_eu_tf_full, accuracy_top10_cdist_eu_tf_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854e61f-0b96-4786-a2fb-262749edd595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcd288-dc37-4d68-b93e-acff3c5106d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.concat([train_df['question_1'],train_df['question_2']],axis=0).drop_duplicates(keep='first')\n",
    "questions_embeddings = model.encode(questions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e507e0-ccf3-44cd-b3c1-320b1aa2c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = questions.sample().values[0]\n",
    "\n",
    "inputs = tokenizer.encode(q, return_tensors=\"pt\")\n",
    "outputs = model_translate.generate(inputs, max_length=1000, num_beams=5, early_stopping=False)\n",
    "translated_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "question_embedding = model.encode(q)\n",
    "predict = np.array([util.cos_sim(questions_embeddings, question_embedding)])\n",
    "\n",
    "N = 11\n",
    "values_top = np.argsort(predict.reshape(-1))[::-1][:N+1]\n",
    "\n",
    "df_ = pd.DataFrame(columns=['questions', 'translated_questions','cos_sim']) \n",
    "for val in values_top:\n",
    "    if questions.tolist()[val] == q:\n",
    "        pass        \n",
    "    else: \n",
    "        q_ = questions.tolist()[val]\n",
    "        cos_sim_ = predict.reshape(-1)[val].round(1)\n",
    "        translated_q_ = translate_text(q_)\n",
    "        df_.loc[val] = [q_,translated_q_,cos_sim_]\n",
    "\n",
    "print(q, translated_question)\n",
    "display(df_[['questions', 'translated_questions','cos_sim']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756c5ae",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Это прекрасно!!))))  ✅:</b>\n",
    "    \n",
    "    \n",
    "    \n",
    "[Напоминает бессмертную классику](http://lib.ru/ANEKDOTY/mouse_driver.txt)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6f492-6751-4bb9-8695-3f29bb2aa33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.query('(question_1 == @q|question_2 == @q)')[['question_1','question_2','label','cos_sim_mLM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ef62f-fc99-4486-b99f-936feda03c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5917c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'../{DATA_DIR}/train_df_processed.csv')\n",
    "train_df = pd.read_csv(f'../{DATA_DIR}/train_df_processed.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fefe47-cb7e-4033-b20b-c0f9439e4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7585790-8d67-432f-83a2-9b3e00abdf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60926afb",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    "<b>Вывод ревьюера:</b>\n",
    "    \n",
    "У тебя получилась отличная работа. Проведено интересное исследование, есть готовое работающее приложение. \n",
    "    \n",
    "Мне очень понравилось!\n",
    "    \n",
    "Удачи в дальнейших проектах!\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad288e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
